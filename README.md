# 1. Glue Job: Bronze to Silver Data Lake ETL

## Description

This AWS Glue job reads raw data files from S3 (bronze layer), applies basic transformations, and writes the processed data into Hudi tables (silver layer) registered in the Glue Catalog.

### Steps

1. **Read Input Data**  
   Reads three input files from S3 and creates a DataFrame for each:
   - customers
   - products
   - transactions

2. **Transform Data**  
   Applies basic string transformations and adds a `load_date` column to each DataFrame.

3. **Write to Hudi Tables**  
   Writes each DataFrame to a Hudi table in S3 and registers the tables in the Glue Catalog.

## Parameters

- `--conf spark.serializer=org.apache.spark.serializer.KryoSerializer`
- `--conf spark.sql.hive.convertMetastoreParquet=false`
- `--datalake-formats hudi`
- `--bucket_name <your_bucket_name>`
- `--ip_path <input_raw_files_path>`
- `--op_path <output_silver_files_path>`

## Notes

- Hardcoded values for database and table names are used in the code.  
  You can modify these as needed.
- The job uses Hudi for upsert operations and partitioning by `load_date`.

## Usage

1. Update the parameters with your S3 bucket and paths.
2. Run the Glue job using the provided script
3. The processed data will be available in the specified output path as Hudi tables.


# 2. EMR_job.py - Data Lake Analytics Pipeline

## Description
This script processes data in AWS EMR using PySpark and Hudi. It reads data from the "data-lake" (SILVER layer) generated by a previous Glue job, performs analytics transformations, and writes results to the "analytics" (GOLD layer) in Hudi tables.

### Steps
**Read Data**: Reads three Hudi tables (customers, products, transactions) from the S3 "data-lake" path and creates DataFrames for each.
**Customer Metrics**: Calculates customer purchase metrics (total transactions, total spent, average transaction amount, first/last purchase).
**Product Analytics**: Calculates product analytics (total units sold, total revenue, average price, unique customers per category/subcategory).
**Write Results**: Adds a process_date column and writes the two aggregate metrics into two new Hudi tables in the S3 "analytics" path.

## Input Parameters
S3 bucket name: The name of the S3 bucket containing the data-lake and analytics layers.
S3 output path for Hudi tables: The path within the bucket for Hudi tables.

## Usage
Run the script using spark-submit with the required JARs and Spark/Hudi configurations:
- spark-submit --deploy-mode cluster \
-  --jars /usr/lib/hudi/hudi-spark-bundle.jar \
-   --conf "spark.serializer=org.apache.spark.serializer.KryoSerializer" \
-   --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog" \
-   --conf "spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension" \
-   EMR_job.py <s3_bucket_name> <s3_op_path_for_hudi_table>




